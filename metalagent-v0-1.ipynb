{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/amirparvardi/metalagent-v0-1?scriptVersionId=207515555\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"26bc232b","metadata":{"papermill":{"duration":0.005654,"end_time":"2024-11-15T08:43:45.076456","exception":false,"start_time":"2024-11-15T08:43:45.070802","status":"completed"},"tags":[]},"source":["## MetalAgent: Metal Music Recommender System\r\n","\n","MetalAgent is a Kaggle notebook integrated with GitHub, designed to act as a personalized recommendation system for metal music enthusiasts. Using LangGraph and LangChain, MetalAgent provides curated recommendations, explores subgenres, and engages in interactive conversations about recent releases\n","\r\n"]},{"cell_type":"markdown","id":"e42274af","metadata":{"papermill":{"duration":0.004674,"end_time":"2024-11-15T08:43:45.086244","exception":false,"start_time":"2024-11-15T08:43:45.08157","status":"completed"},"tags":[]},"source":["## Get set up\n","\n","Start by installing and importing the LangGraph SDK and LangChain support for the Gemini API."]},{"cell_type":"code","execution_count":1,"id":"ad730ee8","metadata":{"execution":{"iopub.execute_input":"2024-11-15T08:43:45.097551Z","iopub.status.busy":"2024-11-15T08:43:45.097167Z","iopub.status.idle":"2024-11-15T08:44:00.997797Z","shell.execute_reply":"2024-11-15T08:44:00.996396Z"},"papermill":{"duration":15.909108,"end_time":"2024-11-15T08:44:01.000195","exception":false,"start_time":"2024-11-15T08:43:45.091087","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.2 which is incompatible.\r\n","jupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n","jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n","kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n","kfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\r\n","libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n","thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\r\n","ydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\r\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install -qU 'langgraph==0.2.45' 'langchain-google-genai==2.0.4'"]},{"cell_type":"markdown","id":"633d9a63","metadata":{"papermill":{"duration":0.0049,"end_time":"2024-11-15T08:44:01.010457","exception":false,"start_time":"2024-11-15T08:44:01.005557","status":"completed"},"tags":[]},"source":["\n","You do not neeed to restart the kernel even if you get the error `ERROR: pip's dependency resolver does not currently take into account all the packages that are installed.`"]},{"cell_type":"markdown","id":"88351b6b","metadata":{"papermill":{"duration":0.004802,"end_time":"2024-11-15T08:44:01.020258","exception":false,"start_time":"2024-11-15T08:44:01.015456","status":"completed"},"tags":[]},"source":["### Set up your API key\n","\n","The `GOOGLE_API_KEY` environment variable can be set to automatically configure the underlying API. This works for both the official Gemini Python SDK and for LangChain/LangGraph. \n","\n","To run the following cell, your API key must be stored it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`.\n","\n","If you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n","\n","To make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook."]},{"cell_type":"code","execution_count":2,"id":"dea0d7cc","metadata":{"execution":{"iopub.execute_input":"2024-11-15T08:44:01.032039Z","iopub.status.busy":"2024-11-15T08:44:01.031616Z","iopub.status.idle":"2024-11-15T08:44:01.336188Z","shell.execute_reply":"2024-11-15T08:44:01.335327Z"},"papermill":{"duration":0.313412,"end_time":"2024-11-15T08:44:01.3386","exception":false,"start_time":"2024-11-15T08:44:01.025188","status":"completed"},"tags":[]},"outputs":[],"source":["import os\n","from kaggle_secrets import UserSecretsClient\n","\n","GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n","os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"]},{"cell_type":"markdown","id":"a643ab14","metadata":{"papermill":{"duration":0.004937,"end_time":"2024-11-15T08:44:01.349806","exception":false,"start_time":"2024-11-15T08:44:01.344869","status":"completed"},"tags":[]},"source":["If you received an error response along the lines of `No user secrets exist for kernel id ...`, then you need to add your API key via `Add-ons`, `Secrets` **and also** *enable* it."]},{"cell_type":"markdown","id":"22fe040b","metadata":{"papermill":{"duration":0.004985,"end_time":"2024-11-15T08:44:01.360279","exception":false,"start_time":"2024-11-15T08:44:01.355294","status":"completed"},"tags":[]},"source":["## Key concepts\n","\n","LangGraph applications are built around a **graph** structure. As the developer, you define an application graph that models the state transitions for your application. Your app will define a **state** schema, and an instance of that schema is propagated through the graph.\n","\n","Each **node** in the graph represents an action or step that can be taken. Nodes will make changes to the state in some way through code that you define. These changes can be the result of invoking an LLM, by calling an API, or executing any logic that the node defines.\n","\n","Each **edge** in the graph represents a transition between states, defining the flow of the program. Edge transitions can be fixed, for example if you define a text-only chatbot where output is always displayed to a user, you may always transition from `chatbot -> user`. The transitions can also be conditional, allowing you to add branching (like an `if-else` statement) or looping (like `for` or `while` loops).\n","\n","LangGraph is highly extensible and provides a number of features that are not part of this tutorial, such as memory, persistance and streaming. To better understand the key concepts and philophies behind LangGraph, check out their [Conceptual guides](https://langchain-ai.github.io/langgraph/concepts/) and [High-level overview](https://langchain-ai.github.io/langgraph/concepts/high_level/)."]},{"cell_type":"markdown","id":"5a31e7a5","metadata":{"papermill":{"duration":0.004828,"end_time":"2024-11-15T08:44:01.370526","exception":false,"start_time":"2024-11-15T08:44:01.365698","status":"completed"},"tags":[]},"source":["## Define core instructions\n","\n","State is a fundamental concept for a LangGraph app. A state object is passed between every node and transition in the app. Here you define a state object, `RequestState`, that holds the conversation history, a structured request, and a flag indicating if the metalhead has finished placing their request. For simplicity, the \"structure\" in this request is just a list of strings, but this can be expanded to any Python data structure.\n","\n","In Python, the LangGraph state object is a Python [dictionary](https://docs.python.org/3/library/stdtypes.html#dict). You can provide a schema for this dictionary by defining it as a [`TypedDict`](https://docs.python.org/3/library/typing.html#typing.TypedDict).\n","\n","Here you also define the system instruction that the Gemini model will use. You can capture tone and style here, as well as the playbook under which the chatbot should operate."]},{"cell_type":"code","execution_count":3,"id":"bea6f576","metadata":{"execution":{"iopub.execute_input":"2024-11-15T08:44:01.382736Z","iopub.status.busy":"2024-11-15T08:44:01.382324Z","iopub.status.idle":"2024-11-15T08:44:02.34175Z","shell.execute_reply":"2024-11-15T08:44:02.340406Z"},"papermill":{"duration":0.968493,"end_time":"2024-11-15T08:44:02.344148","exception":false,"start_time":"2024-11-15T08:44:01.375655","status":"completed"},"tags":[]},"outputs":[],"source":["from typing import Annotated\n","from typing_extensions import TypedDict\n","\n","from langgraph.graph.message import add_messages\n","\n","\n","class RequestState(TypedDict):\n","    \"\"\"State representing the metalhead's request conversation.\"\"\"\n","\n","    # The chat conversation. This preserves the conversation history\n","    # between nodes. The `add_messages` annotation indicates to LangGraph\n","    # that state is updated by appending returned messages, not replacing\n","    # them.\n","    messages: Annotated[list, add_messages]\n","\n","    # The metalhead's in-progress request.\n","    request: list[str]\n","\n","    # Flag indicating that the order is placed and completed.\n","    finished: bool\n","\n","\n","# The system instruction defines how the MetalAgent is expected to behave and includes\n","# rules for when to call different functions, as well as rules for the conversation, such\n","# as tone and what is permitted for discussion.\n","METALAGENT_SYSINT = (\n","    \"system\",  # 'system' indicates the message is a system instruction.\n","    \"You are a MetalAgent, an interactive metal recommendation system. A metalhead will ask you to talk about the \"\n","    \"recent metal releases in your collection and you will answer any questions about the releases (and only about \"\n","    \"genres and releases - no off-topic discussion, but you can chat about the releases and their band's history). \"\n","    \"The metalhead will make a recommendation request releases from 1 or more genres from your collection, \"\n","    \"which you will structure and send to the recommendation system. \"\n","    \"\\n\\n\"\n","    \"Add items to the metalhead's request with add_to_request, and reset the request with clear_request. \"\n","    \"To see the contents of the request so far, call get_request (this is shown to you, not the user) \"\n","    \"Calling confirm_request will display the requested genres to the user and returns their response to seeing the list.\"\n","    \"Respond with a list of releases from metalhead's requested genres and recommend \"\n","    \"possible subgenres from the GENRE MENU and add them to the request. \"\n","    \"After confirming the requested genres (and possibly subgenres) with the metalhead, call confirm_request and then call place_request. \"\n","    \"Once place_request has returned, print the list of releases in the requested subgenres, thank the metalhead and say goodbye!\",\n",")\n","\n","# This is the message with which the system opens the conversation.\n","WELCOME_MSG = \"Welcome to the MetalAgent recommendation system. Type `q` to quit. What metal genre do you have in mind today?\""]},{"cell_type":"markdown","id":"c5d1ad9d","metadata":{"papermill":{"duration":0.004862,"end_time":"2024-11-15T08:44:02.354138","exception":false,"start_time":"2024-11-15T08:44:02.349276","status":"completed"},"tags":[]},"source":["## Define a single turn chatboot\n","\n","To illustrate how LangGraph works, the following program defines a chatbot node that will execute a single turn in a chat conversation using the instructions supplied.\n","\n","Each node in the graph operates on the state object. The state (a Python dictionary) is passed as a parameter into the node (a function) and the new state is returned. This can be restated as pseudo-code, where `state = node(state)`.\n","\n","Note: For the `chatbot` node, the state is updated by *adding* the new conversation message. The `add_messages` annotation on `RequestState.messages` indicates that messages are *appended* when returned from a node. Typically state is updated by replacement, but this annotation causes `messages` to behave differently."]},{"cell_type":"code","execution_count":4,"id":"1f0380ad","metadata":{"execution":{"iopub.execute_input":"2024-11-15T08:44:02.366132Z","iopub.status.busy":"2024-11-15T08:44:02.365169Z","iopub.status.idle":"2024-11-15T08:44:03.794862Z","shell.execute_reply":"2024-11-15T08:44:03.793943Z"},"papermill":{"duration":1.438357,"end_time":"2024-11-15T08:44:03.797398","exception":false,"start_time":"2024-11-15T08:44:02.359041","status":"completed"},"tags":[]},"outputs":[],"source":["from langgraph.graph import StateGraph, START, END\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","\n","# Try using different models. The `pro` models perform the best, especially\n","# with tool-calling. The `flash` models are super fast, and are a good choice\n","# if you need to use the higher free-tier quota.\n","# Check out the features and quota differences here: https://ai.google.dev/pricing\n","llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n","\n","\n","def chatbot(state: RequestState) -> RequestState:\n","    \"\"\"The chatbot itself. A simple wrapper around the model's own chat interface.\"\"\"\n","    message_history = [METALAGENT_SYSINT] + state[\"messages\"]\n","    return {\"messages\": [llm.invoke(message_history)]}\n","\n","\n","# Set up the initial graph based on our state definition.\n","graph_builder = StateGraph(RequestState)\n","\n","# Add the chatbot function to the app graph as a node called \"chatbot\".\n","graph_builder.add_node(\"chatbot\", chatbot)\n","\n","# Define the chatbot node as the app entrypoint.\n","graph_builder.add_edge(START, \"chatbot\")\n","\n","chat_graph = graph_builder.compile()"]},{"cell_type":"markdown","id":"a9d07448","metadata":{"papermill":{"duration":0.004937,"end_time":"2024-11-15T08:44:03.807619","exception":false,"start_time":"2024-11-15T08:44:03.802682","status":"completed"},"tags":[]},"source":["## Add a human node\n","\n","Instead of repeatedly running the \"graph\" in a Python loop, you can use LangGraph to loop between nodes.\n","\n","The `human` node will display the last message from the LLM to the user, and then prompt them for their next input. Here this is done using standard Python `print` and `input` functions.\n","\n","The `chatbot` node function has also been updated to include the welcome message to start the conversation."]},{"cell_type":"code","execution_count":5,"id":"f50d03f4","metadata":{"execution":{"iopub.execute_input":"2024-11-15T08:44:03.820188Z","iopub.status.busy":"2024-11-15T08:44:03.819125Z","iopub.status.idle":"2024-11-15T08:44:03.829529Z","shell.execute_reply":"2024-11-15T08:44:03.82857Z"},"papermill":{"duration":0.018883,"end_time":"2024-11-15T08:44:03.831697","exception":false,"start_time":"2024-11-15T08:44:03.812814","status":"completed"},"tags":[]},"outputs":[],"source":["from langchain_core.messages.ai import AIMessage\n","\n","\n","def human_node(state: RequestState) -> RequestState:\n","    \"\"\"Display the last model message to the user, and receive the user's input.\"\"\"\n","    last_msg = state[\"messages\"][-1]\n","    print(\"Model:\", last_msg.content)\n","\n","    user_input = input(\"User: \")\n","\n","    # If it looks like the user is trying to quit, flag the conversation\n","    # as over.\n","    if user_input in {\"q\", \"quit\", \"exit\", \"goodbye\"}:\n","        state[\"finished\"] = True\n","\n","    return state | {\"messages\": [(\"user\", user_input)]}\n","\n","\n","def chatbot_with_welcome_msg(state: RequestState) -> RequestState:\n","    \"\"\"The chatbot itself. A wrapper around the model's own chat interface.\"\"\"\n","\n","    if state[\"messages\"]:\n","        # If there are messages, continue the conversation with the Gemini model.\n","        new_output = llm.invoke([METALAGENT_SYSINT] + state[\"messages\"])\n","    else:\n","        # If there are no messages, start with the welcome message.\n","        new_output = AIMessage(content=WELCOME_MSG)\n","\n","    return state | {\"messages\": [new_output]}\n","\n","\n","# Start building a new graph.\n","graph_builder = StateGraph(RequestState)\n","\n","# Add the chatbot and human nodes to the app graph.\n","graph_builder.add_node(\"chatbot\", chatbot_with_welcome_msg)\n","graph_builder.add_node(\"human\", human_node)\n","\n","# Start with the chatbot again.\n","graph_builder.add_edge(START, \"chatbot\")\n","\n","# The chatbot will always go to the human next.\n","graph_builder.add_edge(\"chatbot\", \"human\");"]},{"cell_type":"markdown","id":"3a74364f","metadata":{"papermill":{"duration":0.004739,"end_time":"2024-11-15T08:44:03.841625","exception":false,"start_time":"2024-11-15T08:44:03.836886","status":"completed"},"tags":[]},"source":["Before you can run this, note that if you added an edge from `human` back to `chatbot`, the graph will cycle forever as there is no exit condition. One way to break the cycle is to add a check for a human input like `q` or `quit` and use that to break the loop.\n","\n","In LangGraph, this is achieved with a conditional edge. This is similar to a regular graph transition, except a custom function is called to determine which edge to traverse.\n","\n","Conditional edge functions take the state as input, and return a string representing the name of the node to which it will transition."]},{"cell_type":"code","execution_count":6,"id":"4dd0f074","metadata":{"execution":{"iopub.execute_input":"2024-11-15T08:44:03.85335Z","iopub.status.busy":"2024-11-15T08:44:03.852941Z","iopub.status.idle":"2024-11-15T08:44:03.859579Z","shell.execute_reply":"2024-11-15T08:44:03.858626Z"},"papermill":{"duration":0.015091,"end_time":"2024-11-15T08:44:03.861692","exception":false,"start_time":"2024-11-15T08:44:03.846601","status":"completed"},"tags":[]},"outputs":[],"source":["from typing import Literal\n","\n","\n","def maybe_exit_human_node(state: RequestState) -> Literal[\"chatbot\", \"__end__\"]:\n","    \"\"\"Route to the chatbot, unless it looks like the user is exiting.\"\"\"\n","    if state.get(\"finished\", False):\n","        return END\n","    else:\n","        return \"chatbot\"\n","\n","\n","graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)\n","\n","chat_with_human_graph = graph_builder.compile()"]},{"cell_type":"markdown","id":"055e6611","metadata":{"papermill":{"duration":0.004769,"end_time":"2024-11-15T08:44:03.871662","exception":false,"start_time":"2024-11-15T08:44:03.866893","status":"completed"},"tags":[]},"source":["## Add a \"genre\" menu\n","\n","MetalAgent currently has no awareness of the available genres or releases in metal, so it will hallucinate a genre menu. One option would be to hard-code a genre menu into the system prompt. This would work well, but to simulate a system where the genre menu is more dynamic and could respond to actual metal releases, you will put the menu into a custom tool.\n","\n","There are two types of tools that this system will use. Stateless tools that can be run automatically, and stateful tools that modify the request. The \"get current menu\" tool is stateless, in that it does not make any changes to the live request, so it can be called automatically.\n","\n","In a LangGraph app, you can annotate Python functions as tools by applying the `@tools` annotation.\n"]},{"cell_type":"code","execution_count":7,"id":"8ec7035b","metadata":{"execution":{"iopub.execute_input":"2024-11-15T08:44:03.883262Z","iopub.status.busy":"2024-11-15T08:44:03.882893Z","iopub.status.idle":"2024-11-15T08:44:03.905086Z","shell.execute_reply":"2024-11-15T08:44:03.904217Z"},"papermill":{"duration":0.03084,"end_time":"2024-11-15T08:44:03.907458","exception":false,"start_time":"2024-11-15T08:44:03.876618","status":"completed"},"tags":[]},"outputs":[],"source":["from langchain_core.tools import tool\n","\n","# Define the path to subgenres and recent releases\n","Subgenres_path = '/kaggle/input/metal-subgenres/subgenres.txt'\n","with open(Subgenres_path, 'r', encoding='utf-8') as file:\n","        subgenres_text = file.read()\n","\n","NOV_08_2024_path = '/kaggle/input/week-of-november-08-2024/albums_Nov02-Nov08.txt'\n","with open(NOV_08_2024_path, 'r', encoding='utf-8') as file:\n","        NOV_08_2024_releases = file.read()\n","    \n","\n","@tool\n","def get_menu() -> str:\n","    \"\"\"Provide the latest up-to-date genre menu and recent releases.\"\"\"\n","    # Note that this is just hard-coded text, but you could connect this to a live releases\n","    # database, or you could use Gemini's multi-modal capabilities and take live photos of\n","    # your favourite concert's band-list or the songs in a playlist and assmble them into an input.\n","\n","    return f\"\"\"\n","    GENRE MENU: {subgenres_text}\n","    \n","    Recent Releases:\n","    Week of November 08, 2024: {NOV_08_2024_releases}\n","    \"\"\""]},{"cell_type":"markdown","id":"71ad5e6e","metadata":{"papermill":{"duration":0.004873,"end_time":"2024-11-15T08:44:03.917774","exception":false,"start_time":"2024-11-15T08:44:03.912901","status":"completed"},"tags":[]},"source":["Now add the new tool to the graph. The `get_menu` tool is wrapped in a [`ToolNode`](https://langchain-ai.github.io/langgraph/reference/prebuilt/#toolnode) that handles calling the tool and passing the response as a message through the graph. The tools are also bound to the `llm` object so that the underlying model knows they exist. As you now have a different `llm` object to invoke, you need to update the `chatbot` node so that it is aware of the tools."]},{"cell_type":"code","execution_count":8,"id":"df99f0c1","metadata":{"execution":{"iopub.execute_input":"2024-11-15T08:44:03.929961Z","iopub.status.busy":"2024-11-15T08:44:03.929521Z","iopub.status.idle":"2024-11-15T08:44:04.09641Z","shell.execute_reply":"2024-11-15T08:44:04.094824Z"},"papermill":{"duration":0.176102,"end_time":"2024-11-15T08:44:04.099078","exception":false,"start_time":"2024-11-15T08:44:03.922976","status":"completed"},"tags":[]},"outputs":[],"source":["from langgraph.prebuilt import ToolNode\n","\n","\n","# Define the tools and create a \"tools\" node.\n","tools = [get_menu]\n","tool_node = ToolNode(tools)\n","\n","# Attach the tools to the model so that it knows what it can call.\n","llm_with_tools = llm.bind_tools(tools)\n","\n","\n","def maybe_route_to_tools(state: RequestState) -> Literal[\"tools\", \"human\"]:\n","    \"\"\"Route between human or tool nodes, depending if a tool call is made.\"\"\"\n","    if not (msgs := state.get(\"messages\", [])):\n","        raise ValueError(f\"No messages found when parsing state: {state}\")\n","\n","    # Only route based on the last message.\n","    msg = msgs[-1]\n","\n","    # When the chatbot returns tool_calls, route to the \"tools\" node.\n","    if hasattr(msg, \"tool_calls\") and len(msg.tool_calls) > 0:\n","        return \"tools\"\n","    else:\n","        return \"human\"\n","\n","\n","def chatbot_with_tools(state: RequestState) -> RequestState:\n","    \"\"\"The chatbot with tools. A simple wrapper around the model's own chat interface.\"\"\"\n","    defaults = {\"request\": [], \"finished\": False}\n","\n","    if state[\"messages\"]:\n","        new_output = llm_with_tools.invoke([METALAGENT_SYSINT] + state[\"messages\"])\n","    else:\n","        new_output = AIMessage(content=WELCOME_MSG)\n","\n","    # Set up some defaults if not already set, then pass through the provided state,\n","    # overriding only the \"messages\" field.\n","    return defaults | state | {\"messages\": [new_output]}\n","\n","\n","graph_builder = StateGraph(RequestState)\n","\n","# Add the nodes, including the new tool_node.\n","graph_builder.add_node(\"chatbot\", chatbot_with_tools)\n","graph_builder.add_node(\"human\", human_node)\n","graph_builder.add_node(\"tools\", tool_node)\n","\n","# Chatbot may go to tools, or human.\n","graph_builder.add_conditional_edges(\"chatbot\", maybe_route_to_tools)\n","# Human may go back to chatbot, or exit.\n","graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)\n","\n","# Tools always route back to chat afterwards.\n","graph_builder.add_edge(\"tools\", \"chatbot\")\n","\n","graph_builder.add_edge(START, \"chatbot\")\n","graph_with_menu = graph_builder.compile()"]},{"cell_type":"markdown","id":"84362401","metadata":{"papermill":{"duration":0.005008,"end_time":"2024-11-15T08:44:04.109385","exception":false,"start_time":"2024-11-15T08:44:04.104377","status":"completed"},"tags":[]},"source":["Now run the new graph to see how the model uses the genre menu.\n","\n","**You must uncomment the line containing `.invoke(...)` to run this step.**"]},{"cell_type":"code","execution_count":9,"id":"b5fd85f9","metadata":{"execution":{"iopub.execute_input":"2024-11-15T08:44:04.12117Z","iopub.status.busy":"2024-11-15T08:44:04.120425Z","iopub.status.idle":"2024-11-15T08:44:04.125048Z","shell.execute_reply":"2024-11-15T08:44:04.124047Z"},"papermill":{"duration":0.012775,"end_time":"2024-11-15T08:44:04.12716","exception":false,"start_time":"2024-11-15T08:44:04.114385","status":"completed"},"tags":[]},"outputs":[],"source":["# Image(graph_with_menu.get_graph().draw_mermaid_png())\n","\n","# Remember that you have not implemented ordering yet, so this will loop forever,\n","# unless you input `q`, `quit` or one of the other exit terms defined in the\n","# `human_node`.\n","# Uncomment this line to execute the graph:\n","# user_msg = \"Recommend ten recent black metal releases in diverse subgenres.\"\n","# state = graph_with_menu.invoke({\"messages\": []})\n","\n","# Things to try:\n","# - I'd love an espresso drink, what have you got?\n","# - What teas do you have?\n","# - Can you do a long black? (this is on the menu as an \"Americano\" - see if it can figure it out)\n","# - 'q' to exit.\n","\n","\n","# pprint(state)"]},{"cell_type":"markdown","id":"5f77e8a4","metadata":{"papermill":{"duration":0.00473,"end_time":"2024-11-15T08:44:04.137002","exception":false,"start_time":"2024-11-15T08:44:04.132272","status":"completed"},"tags":[]},"source":["**NOTE**: To save this Kaggle notebook, you will need to comment out the line containing `.invoke(...)` in order for the notebook to Run All cells and finally be able to save your version."]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":6086874,"sourceId":9907298,"sourceType":"datasetVersion"},{"datasetId":6087144,"sourceId":9907664,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"papermill":{"default_parameters":{},"duration":22.429208,"end_time":"2024-11-15T08:44:04.862183","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-11-15T08:43:42.432975","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}